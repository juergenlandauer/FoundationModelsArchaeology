{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juergenlandauer/FoundationModelsArchaeology/blob/main/Experiment_4_Hillforts_SAM_segmentation/SAM_hillforts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic site segmentation with SAM: English hillforts\n",
        "Author: Juergen Landauer (juergen AT landauer-ai.de)\n",
        "\n",
        "To start, first go to the \"Input parameters\" section below and review or (optionally) adjust parameters. Then run the entire Notebook by choosing Runtime->Run all in the menu above."
      ],
      "metadata": {
        "id": "k0lt4re3-f_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "eHE3X6UU-uui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first install Meta's SAM, Roboflow's Supervision and other libraries, then the SAM model weights. This could take up to 5 minutes."
      ],
      "metadata": {
        "id": "L30FjOmk-s6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq opencv-python supervision\n",
        "!pip install -Uq 'git+https://github.com/facebookresearch/sam2.git'"
      ],
      "metadata": {
        "id": "pIeoaUNt-0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caution!\n",
        "You might get asked to restart this session here. This is mandatory."
      ],
      "metadata": {
        "id": "pyUSfJSt-7-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input parameters\n",
        "\n",
        "Review all parameters in this section and (optionally) adjust them on the right side. For example, you can upload your own input zip file by providing an URL."
      ],
      "metadata": {
        "id": "4zmV4Xll_DlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feel free to replace this with your own imagery by providing a download URL (e.g. from Google Drive)\n",
        "# Note that the ZIP file must contain only images in its root folder. No sub-directories!\n",
        "\n",
        "INPUT_ZIP_URL = 'https://www.dropbox.com/scl/fi/8eeuwz4ao5s93e6h6msjm/DEM_forts_EN_768_V8_hillshade.zip?rlkey=eriky7xiskvs3z69jnso4qpi7&dl=0' # @param {\"allow-input\":true}"
      ],
      "metadata": {
        "id": "tmM2jorP_KEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wkV75Db9tXF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!rm -rf input output file.zip\n",
        "!mkdir -p input/\n",
        "!wget -O file.zip \"$INPUT_ZIP_URL\"\n",
        "!unzip -q file.zip -d input\n"
      ],
      "metadata": {
        "id": "nTcHQluSEAtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download SAM weights\n",
        "!mkdir -p ../checkpoints/\n",
        "#!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt"
      ],
      "metadata": {
        "id": "QZ2TZDt_-_-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYnn6402-JhQ"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHRLQPV4WKd1"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIcNq3IiXufS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svThmVIGZZAc"
      },
      "source": [
        "**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZAXCVT0ZWkd"
      },
      "outputs": [],
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLeXwS2UQU5y"
      },
      "source": [
        "## Load model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v86rUA2o-Jhc"
      },
      "outputs": [],
      "source": [
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvgsf08QRZo"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/../checkpoints/sam2.1_hiera_small.pt\"\n",
        "CONFIG = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIA7L7FoVWqE"
      },
      "source": [
        "## Automated mask generation\n",
        "\n",
        "Since SAM 2 can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image.\n",
        "\n",
        "The class `SAM2AutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-X6PaJu-Jhe"
      },
      "outputs": [],
      "source": [
        "mask_generator = SAM2AutomaticMaskGenerator(\n",
        "    sam2_model,\n",
        "    points_per_side=64,\n",
        "    points_per_batch=128,\n",
        "    pred_iou_thresh=0.7,\n",
        "    stability_score_thresh=0.92,\n",
        "    stability_score_offset=0.7,\n",
        "    crop_n_layers=1,\n",
        "    box_nms_thresh=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHrJV4HmavcP"
      },
      "source": [
        "**NOTE:** OpenCV loads images in BGR format by default, so we convert to RGB for compatibility with the mask generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t01R9cy-Jhh"
      },
      "outputs": [],
      "source": [
        "def generate (image):\n",
        "    image_rgb = image#cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    sam2_result = mask_generator.generate(image_rgb)\n",
        "    return sam2_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v1lpyaVWhHY"
      },
      "source": [
        "### Output format\n",
        "\n",
        "`SAM2AutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
        "\n",
        "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
        "* `area` - `[int]` - the area of the mask in pixels\n",
        "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
        "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
        "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
        "* `stability_score` - `[float]` - an additional measure of mask quality\n",
        "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyl2Ldc_Wvtm"
      },
      "source": [
        "### Results visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQM2WlU-Jhj"
      },
      "outputs": [],
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "def annotate(image_bgr, sam2_result, filename):\n",
        "    detections = sv.Detections.from_sam(sam_result=sam2_result)\n",
        "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "    sv.plot_images_grid(\n",
        "        images=[image_bgr, annotated_image],\n",
        "        grid_size=(1, 2),\n",
        "        titles=['source image', 'segmented image']\n",
        "    )\n",
        "\n",
        "    if filename is not None:\n",
        "        cv2.imwrite(filename, annotated_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgzoEpZe-Jh2"
      },
      "outputs": [],
      "source": [
        "def load_rgb(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kOcWyhi-Jh3"
      },
      "outputs": [],
      "source": [
        "def preprocessSnippetNONE(image: np.ndarray): return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNDyPdxp-Jh3"
      },
      "outputs": [],
      "source": [
        "myfilter = preprocessSnippetNONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddWVB58-Jh3"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_img(f):\n",
        "    image = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
        "    image = np.stack([image]*3, axis=2).astype(np.uint8) # make it greyscale 3 channel\n",
        "    image = myfilter(image).astype(np.uint8)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrkun5k-Jh4"
      },
      "source": [
        "## process folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"./input/\""
      ],
      "metadata": {
        "id": "NS-dLMirE5tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAgIdLZH-Jh5"
      },
      "outputs": [],
      "source": [
        "OUTPATH = Path(\"./output\")\n",
        "os.makedirs(OUTPATH, exist_ok=True)\n",
        "\n",
        "files = sorted(glob(IMAGE_PATH+\"**\", recursive=True))\n",
        "#files = files[:2] # for testing\n",
        "len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQyM6jbT-Jh5"
      },
      "outputs": [],
      "source": [
        "sz = 768\n",
        "default_box = [\n",
        "    {'x': sz//2, 'y': sz//2, 'width': 0, 'height': 0, 'label': ''},\n",
        "]\n",
        "\n",
        "input_point = np.array([\n",
        "    [sz//2, sz//2]\n",
        "])\n",
        "input_label = np.ones(input_point.shape[0])\n",
        "input_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se_BNRaN-Jh5"
      },
      "outputs": [],
      "source": [
        "predictor = SAM2ImagePredictor(sam2_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uflcyXIH-Jh6"
      },
      "outputs": [],
      "source": [
        "for f in tqdm(files):\n",
        "    p = Path(f)\n",
        "    image_bgr = read_img(p)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    predictor.set_image(image_rgb)\n",
        "    masks, scores, logits = predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=True,\n",
        "    )\n",
        "\n",
        "    for s in range(len(scores)):\n",
        "        if scores[s] == max(scores): # got it\n",
        "            mask = masks[s].astype(np.uint8)\n",
        "            break\n",
        "\n",
        "    redImg = np.zeros_like(image_bgr)\n",
        "    redImg[:,:] = (0, 255, 255)\n",
        "    redMask = cv2.bitwise_and(redImg, redImg, mask=mask)\n",
        "\n",
        "    overlay = cv2.addWeighted(redMask, .3, image_bgr, 1-.3, 0)\n",
        "\n",
        "    sv.plot_images_grid(\n",
        "        images=[image_bgr, overlay],\n",
        "        titles=[\"orig\", f\"score: {scores[s]:.2f}\"],\n",
        "        grid_size=(1, 2),\n",
        "        size=(12, 6)\n",
        "    )\n",
        "    # save mask and overlay\n",
        "    cv2.imwrite(OUTPATH/(\"orig_\"+p.name), image_bgr)\n",
        "    cv2.imwrite(OUTPATH/(\"SAM_\"+p.name), overlay)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export results for download\n",
        "We now open a file download dialog for the output.zip. Simply store the output in your local computer. Done :-)"
      ],
      "metadata": {
        "id": "KxowZQvJ__zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output.zip output\n",
        "colabfiles.download('output.zip')"
      ],
      "metadata": {
        "id": "LurRLxuFAB86"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}